<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Embodied Scene Understanding for Vision Language Models via MetaVQA | MetaDriverse</title>
    <meta name="author" content="  MetaDriverse">
    <meta name="description" content="&lt;h3&gt;CVPR 2025&lt;/h3&gt;">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">

    <!-- Chenda additional setting -->
    <link rel="stylesheet" id="elementor-frontend-css" href="https://template.makedreamwebsite.com/wp-content/plugins/elementor/assets/css/frontend-lite.min.css?ver=3.9.0" media="all">
    <link rel="stylesheet" id="elementor-post-1391-css" href="https://template.makedreamwebsite.com/wp-content/uploads/elementor/css/post-1391.css?ver=1670496920" media="all">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.1/css/all.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/fullPage.js/3.1.2/fullpage.min.css">
    <script src="https://unpkg.com/infinite-scroll@3/dist/infinite-scroll.pkgd.min.js"></script>
    <!-- Magnific Popup CSS and JS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css">
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css" rel="stylesheet">
    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%98%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://metadriverse.github.io//metavqa/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><img class="navbar-logo" src="/assets/img/UCLAxMetaDriverse-Logo-3.png" alt="Lab logo"></a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">Projects</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/metadrive/">MetaDrive</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/scenarionet/">ScenarioNet</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/team/">Zhou Lab at UCLA</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container-fluid mt-5">
      
      <div class="row justify-content-md-center">
        <div class="col-sm-8">
          <!-- research.html -->
<div class="post">
    <header class="post-header center-text">
        <h1 class="post-title">Embodied Scene Understanding for Vision Language Models via MetaVQA</h1>
        <p class="post-description"></p>
<h3>CVPR 2025</h3>
        
        <div class="col-12 md-5" style="margin-bottom: 2rem;">
            <h6 style="text-align: center; font-size: 1.2rem; margin-bottom: 0.5rem;">
                
                <a href=""><b>Weizhen Wang</b></a>
                <sup></sup>
                , 
                
                <a href="https://chendaduan.com" rel="external nofollow noopener" target="_blank"><b>Chenda Duan</b></a>
                <sup></sup>
                , 
                
                <a href="https://pengzhenghao.github.io/" rel="external nofollow noopener" target="_blank"><b>Zhenghao Peng</b></a>
                <sup></sup>
                , 
                
                <a href="https://scholar.google.com/citations?user=ZQoOjaIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><b>Yuxin Liu</b></a>
                <sup></sup>
                , 
                
                <a href="https://boleizhou.github.io/" rel="external nofollow noopener" target="_blank"><b>Bolei Zhou</b></a>
                <sup></sup>
                
                
            </h6>
            <h6 style="text-align: center; font-size: 1rem;">
                
                <sup></sup>
                University of California, Los Angeles
                
                
            </h6>
        </div>
        
        <div class="col-12 md-5" align="center">
            <h6 style="text-align: center; font-size: 1.2rem;">
                
                <a href="https://github.com/WeizhenWang-1210/MetaVQA" rel="external nofollow noopener" target="_blank"><b>Code</b></a>
                
                
                
                
                
                
                | <a href="https://arxiv.org/abs/2501.09167" rel="external nofollow noopener" target="_blank"><b>Paper</b></a>
                
            </h6>
        </div>
    </header>


    <article>
        
        
        <div class="research-section">
            <style>
.custom-heading {
  font-size: 1.5em;
  font-weight: bold;
  margin-bottom: 10px; /* Adjust this value as needed */
}
.white-background {
    background-color: white;
    display: block; /* Changed from inline-block if you want it to take the full width available */
    width: 100%; /* Ensures it takes the full width of its parent container */
    overflow: hidden; /* This will prevent any overflow outside this div */
    padding: 5px;
}
.white-background img {
    width: 100%; /* Makes the image responsive */
    height: auto; /* Keeps the image's aspect ratio intact */
}
.video-container {
  position: relative;
  max-width: 100%; /* Adjust this value to control the maximum width of the video container */
  margin: 0 auto 0; /* Optional: center the video container horizontally */
}
.video-container video {
  display: block;
  margin: 0 auto;
  max-width: 100%;
  max-height: 100%;
}
.video-grid {
    display: grid;
    grid-template-columns: 1fr 1fr; /* Creates two columns */
    grid-gap: 20px; /* Space between videos */
}
.video iframe {
    width: 100%; /* Ensures iframe takes the full width of the container */
    height: 250px; /* Fixed height for all videos */
}

@media (max-width: 600px) {
    .video-grid {
        grid-template-columns: 1fr; /* Stacks videos into a single column on small screens */
    }
}
.gif img {
    width: 100%; /* Ensures the GIFs fill the cells */
    height: auto; /* Maintains the aspect ratio */
}
.button {
    text-decoration: none;
    border: 2px solid #d9d9d9;
    border-radius: 50px;
    padding: 10px 20px;
    font-size: 16px;
    color: #333;
    background-color: white;
    transition: all 0.3s ease;
}
.button:hover {
    background-color: #f0f0f0;
    border-color: #333;
    color: #007bff;
}
table th, 
table td {
    padding: 4px 8px !important;  /* Force smaller padding */
    line-height: 1.2 !important;  /* Control line height */
}

table tr.group-row td {
    padding: 0 !important;
    height: 8px !important;  /* Force smaller divider height */
    line-height: 1 !important;
}

/* Make all table text slightly smaller */
table, 
table th, 
table td {
    font-size: 14px !important;
}

/* Keep other styles but make them more specific */
table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
}

table th {
    background-color: #f2f2f2;
    font-weight: bold;
}

table tr:not(.group-row):hover {
    background-color: #f8f9fa;
}
</style>

<!-- <div class="buttons" style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
    <a href="#" class="button">PDF (TBD)</a>
    <a href="#" class="button">Arxiv (TBD)</a>
    <a href="#" class="button">GitHub (TBD)</a>
    <a href="#" class="button">Huggingface (TBD)</a>
    <a href="#" class="button">Inspect Dataset (TBD)</a>
</div> -->
<div class="research-section">
    <div class="white-background">
        <img src="../assets/img/metavqa/teaser.png">
    </div>
</div>
<div class="research-section">
    <ul style="list-style-type: none; padding-left: 0;">
      <strong>TL,DR: </strong> MetaVQA is a holistic benchmark for evaluating and enhancing <strong>general-purpose VLM as embodied agent</strong>.
    </ul>
</div>

<div class="research-section">
    <h3 style="text-align: center">Design Choice</h3>
    <div class="white-background">
        <img src="../assets/img/metavqa/som.png">
    </div>
   <strong>Multiple-Choice with Set-of-Marks (SoM)</strong> To effectively communicate with general-purpose vision-language models (VLMs) in visual question answering (VQA) tasks, we draw an analogy to students taking standardized tests, where clear and intuitive instructions ensure fair evaluation. Existing works rely on heterogeneous conventions, like associating pixel coordinates with image regions or using continuous versus discretized spatial information. These conventions, however, are rare in pre-training corpora dominated by human-created Internet data, making it difficult to distinguish whether poor zero-shot performance arises from a lack of scene understanding or unfamiliarity with these conventions. To address these challenges, we adopt the SoM prompting technique, which enhances visual grounding and provides unambiguous, labeled references. By formulating questions in a multiple-choice format with discretized spatial and dynamic information, we ensure fair, intuitive, and zero-shot evaluations. Our benchmark includes 30 question types that comprehensively assess spatial reasoning and embodied understanding through diverse real-world and simulated scenarios, validated further via closed-loop driving simulations.
</div>
<div class="research-section">
    <h3 style="text-align: center">Dataset Compositions</h3>
    <div class="white-background">
        <img src="../assets/img/metavqa/metavqa_distribution.png">
    </div>
   <p><strong>Left</strong>: Distribution of the question types. <strong>Right</strong>: Example for each question supertype.</p>

   <p>MetaVQA Dataset consists of a large corpus of multiple-choice questions, which contains 4,305,450 questions using 442,102 annotated frames extracted from 400 nuScenes scenarios and 6,900 Waymo scenarios covering 59,682 seconds (16.5 hours) of driving log. The questions can be categorized into three supercategories: <strong>spatial</strong> questions, <strong>embodied</strong> questions, and <strong>grounding</strong> questions. The former two supercategories cover the two facets of embodied scene understanding: spatial awareness and embodied understanding,  and the latter one diagnoses VLMs' capabilities to associate marked objects in the observation with textual referral.</p>
</div>
<div class="research-section">
    <h3 style="text-align: center">Learned Embodied Scene Understanding</h3>
    <div class="white-background">
        <img src="/assets/img/metavqa/closed_loop-demo_5.png">
    </div>
    <div class="video-container">
        <video loop="" autoplay="" muted="" playsinline="" controls="">
            <source src="/assets/img/metavqa/closed_1.mp4" type="video/mp4"></source>
        </video>
    </div>
    <div class="video-container">
        <video loop="" autoplay="" muted="" playsinline="" controls="">
            <source src="/assets/img/metavqa/closed_2.mp4" type="video/mp4"></source>
        </video>
    </div>
    <strong>Qualitative result of closed-loop evaluation.</strong> We evaluated the performance of vision-language models in a closed-loop driving set up. Case 1 compares the performance of fine-tuned Llama3.2 (left) versus its zero-shot counterpart (right) in the same scenario. Case 2 compares the performance of fine-tuned Llama3.2 (left) versus fine-tuned Qwen2 (right). As shown, fine-tuned Llama3.2 gains elevated situational awareness and can avoid collision. It also demonstrates superior safety capability compared to its trained peers.
    <div class="white-background">
        <img src="../assets/img/metavqa/learned-DEMO_2.png">
    </div>
    <strong>Improved embodied scene understanding after fine-tuning</strong> of InternVL2-8B on the withheld training set. The VLM demonstrates improved spatial understanding and embodied knowledge after learning the MetaVQA Dataset. In addition, the model attains better grounding capability.
</div>

<div class="research-section">
    <h3 style="text-align: center">VQA Benchmarks</h3>
    <div class="white-background">
        <img src="../assets/img/metavqa/radar-figure-0106.png">
    </div>
    <div class="white-background">
        <img src="../assets/img/metavqa/vqabenchmark.png">
    </div>
    <strong>Visual question answering benchmark</strong>. Performance comparison of different models on overall, simulation-only-part, and real-only-part of the withheld test sets. The parsing failure rate is also provided. Models report consistent improvements after fine-tuning, with InternVL2-8B achieving the best performance.
    
</div>
<div class="research-section">
    <h3 style="text-align: center">Close-Loop Evaluation</h3>
    <div class="white-background">
        <img src="../assets/img/metavqa/closed_loop_pipeline.png">
    </div>
    <strong>Formulation of closed-loop evaluation</strong>. At every five simulation steps (0.5 seconds wall time), the evaluated VLM is provided with annotated observations and current navigation command. The chosen action will be fed into the simulation.
    <div class="white-background">
        <img src="../assets/img/metavqa/closeloop.png">
    </div>
    <strong>Quantitative result of closed-loop evaluation</strong>. Despite not being directly trained on the driving task, VLMs report improvements in closed-loop metrics after learning the MetaVQA Dataset, in addition to better VQA accuracy. This correlation suggests that the MetaVQA Dataset contains generalizable embodied knowledge that could be easily learned and transferred to the downstream application domain (in this case, self-driving).
</div>

<div class="research-section">
    <h3 style="text-align: center">Reference</h3>
<pre><code class="language-plain">@article{wang2025metavqa,
  title={Embodied Scene Understanding for Vision Language Models via MetaVQA},
  author={Wang, Weizhen and Duan, Chenda and Peng, Zhenghao and Liu, Yuxin and Zhou, Bolei},
  journal={arXiv preprint arXiv:2501.09167},
  year={2025}
}</code></pre>
</div>


        </div>
        

    </article>
</div>

<script defer src="../assets/js/copy_code.js" type="text/javascript"></script>
        </div>
      </div>
      
    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2025   MetaDriverse. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
